{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T18:05:37.302601Z",
     "start_time": "2019-12-29T18:05:36.768150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/skywatcher/datasets/data/cityscapes\u001b[00m\n",
      "├── \u001b[01;34mgtCoarse\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain_extra\u001b[00m\n",
      "│   └── \u001b[01;34mval\u001b[00m\n",
      "├── \u001b[01;34mgtFine\u001b[00m\n",
      "│   ├── \u001b[01;34mtest\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain\u001b[00m\n",
      "│   └── \u001b[01;34mval\u001b[00m\n",
      "└── \u001b[01;34mleftImg8bit\u001b[00m\n",
      "    ├── \u001b[01;34mtest\u001b[00m\n",
      "    ├── \u001b[01;34mtrain\u001b[00m\n",
      "    ├── \u001b[01;34mtrain_extra\u001b[00m\n",
      "    └── \u001b[01;34mval\u001b[00m\n",
      "\n",
      "13 directories\n"
     ]
    }
   ],
   "source": [
    "!tree -d -L 2 ~/datasets/data/cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-21T16:05:27.572474Z",
     "start_time": "2019-12-21T16:05:27.555288Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's do some data preprocessing\n",
    "\n",
    "#Import the packages\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "'''\n",
    "The task of this function will be to take the extracted cityscapes data\n",
    "given by source, which I will assume is a directory containing the following\n",
    "structure:\n",
    "cityscapes\n",
    "├── gtCoarse\n",
    "│   ├── train\n",
    "│   ├── train_extra\n",
    "│   └── val\n",
    "├── gtFine\n",
    "│   ├── test\n",
    "│   ├── train\n",
    "│   └── val\n",
    "└── leftImg8bit\n",
    "    ├── test\n",
    "    ├── train\n",
    "    ├── train_extra\n",
    "    └── val\n",
    "\n",
    "This will be the structure if you downloaded the cityscapes dataset and extracted it.\n",
    "It will move all the colour images from train to a directory called training_masks. They\n",
    "will be grabbed from the gtFine train directory. If there are any of the Coarse or trainextra\n",
    "directories in here, then the function will take these too. \n",
    "\n",
    "@param source - the directory for cityscapes that has the structure mentioned above (This is best done as an absolute path)\n",
    "\n",
    "'''\n",
    "def cs_preprocessing_files(source):\n",
    "    #Create all the variables\n",
    "    fine_masks = source + '/train/masks'\n",
    "    coarse_masks = source + '/train/weak_masks'\n",
    "    fine_train = source + '/train/images'\n",
    "    coarse_train = source + '/train/weak_images'\n",
    "    fine_masks_val = source + '/val/masks'\n",
    "    coarse_masks_val = source + '/val/weak_masks'\n",
    "    fine_val = source + '/val/images'\n",
    "    coarse_val = source + '/val/weak_images'\n",
    "    \n",
    "    #Create all the directories\n",
    "    os.makedirs(fine_masks)\n",
    "    os.makedirs(coarse_masks)\n",
    "    os.makedirs(fine_train)\n",
    "    os.makedirs(coarse_train)\n",
    "    os.makedirs(fine_masks_val)\n",
    "    os.makedirs(coarse_masks_val)\n",
    "    os.makedirs(fine_val)\n",
    "    os.makedirs(coarse_val)\n",
    "    \n",
    "    #Go through the directory\n",
    "    for root, dirs, files in os.walk(source):\n",
    "        #Save on walking time\n",
    "        if 'test' in root:\n",
    "            continue\n",
    "        #If we are looking at a dir in the gtFine or gtCoarse dirs\n",
    "        elif 'gtFine' in root or 'gtCoarse' in root:\n",
    "            #Get the files in that dir\n",
    "            for file in files:\n",
    "                #Create a string for easy reference of the file\n",
    "                file_str = '/' + file\n",
    "                #If its got color in the file name, then its a mask\n",
    "                if 'color' in file:\n",
    "                    #If we're doing the fine annotations\n",
    "                    if 'gtFine' in root:\n",
    "                        #Then if its the training masks\n",
    "                        if 'train' in root:\n",
    "                            #Write to training directory\n",
    "                            copyfile(root + file_str, fine_masks + file_str)\n",
    "                        elif 'val' in root:\n",
    "                            #Otherwise if its the validation dir, write to val instead\n",
    "                            copyfile(root + file_str, fine_masks_val + file_str)\n",
    "                    else:\n",
    "                        if 'train' in root:\n",
    "                            #Write to training directory\n",
    "                            copyfile(root + file_str, coarse_masks  + file_str)\n",
    "                        elif 'val' in root:\n",
    "                            #Otherwise if its the validation dir, write to val instead\n",
    "                            copyfile(root + file_str, coarse_masks_val + file_str)\n",
    "        #If its the actual images directory we're looking at\n",
    "        elif 'leftImg8bit' in root:\n",
    "            #Get all the files\n",
    "            for file in files:\n",
    "                file_str = '/' + file\n",
    "                #If we are looking at the extra training images\n",
    "                if 'extra' in root:\n",
    "                    if 'train' in root:\n",
    "                        #Copy to just the weak training directory\n",
    "                        copyfile(root + file_str, coarse_train  + file_str)\n",
    "                    elif 'val' in root:\n",
    "                        #Copy to just the weak validation directory\n",
    "                        copyfile(root + file_str, coarse_val + file_str)\n",
    "                else:\n",
    "                    if 'train' in root:\n",
    "                        #Copy to both training directories (As coarse annotations are on all images)\n",
    "                        copyfile(root + file_str, fine_train  + file_str)\n",
    "                        copyfile(root + file_str, coarse_train  + file_str)\n",
    "                    elif 'val' in root:\n",
    "                        #Copy to both validation directories for same reason as training\n",
    "                        copyfile(root + file_str, fine_val  + file_str)\n",
    "                        copyfile(root + file_str, coarse_val + file_str)\n",
    "\n",
    "'''\n",
    "An ImageDataGenerator will be created using keras ready for use in training TF2.0 models\n",
    "like FastSCNN. \n",
    "\n",
    "@param images - The directory where all the training images are kept\n",
    "@param masks - The directory where all the masks are kept\n",
    "@param val_images - The directory where all validation images are kept\n",
    "@param val_masks - The directory where all validation masks are kept\n",
    "@param target_size - Should be a tuple of two numbers, the image size needed for the model should be put here. \n",
    "                     This will be largely dependent on the network you are training.\n",
    "@return ImageDataGenerator - The ImageDataGenerator\n",
    "'''\n",
    "def cs_create_generators(images, masks, val_images, val_masks, target_size):\n",
    "    \n",
    "    # we create two instances with the same arguments\n",
    "    data_gen_args = dict(rotation_range=90,\n",
    "                         width_shift_range=0.1,\n",
    "                         height_shift_range=0.1,\n",
    "                         zoom_range=0.2)\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    # Provide the same seed to the flow methods\n",
    "    seed = 42\n",
    "\n",
    "    #Create the generators\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        images,\n",
    "        class_mode=None,\n",
    "        seed=seed)\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        masks,\n",
    "        class_mode=None,\n",
    "        seed=seed)\n",
    "    \n",
    "    #Create the generators\n",
    "    val_image_generator = image_datagen.flow_from_directory(\n",
    "        val_images,\n",
    "        class_mode=None,\n",
    "        seed=seed)\n",
    "\n",
    "    val_mask_generator = mask_datagen.flow_from_directory(\n",
    "        val_masks,\n",
    "        class_mode=None,\n",
    "        seed=seed)\n",
    "\n",
    "    # combine generators into one which yields image and masks\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    val_generator = zip(val_image_generator, val_mask_generator)\n",
    "    return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T17:16:47.740210Z",
     "start_time": "2019-12-20T14:37:47.670525Z"
    }
   },
   "outputs": [],
   "source": [
    "cs_preprocessing_files('/home/skywatcher/datasets/data/cityscapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T18:05:57.411502Z",
     "start_time": "2019-12-29T18:05:57.245289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/skywatcher/datasets/data/cityscapes\u001b[00m\r\n",
      "├── \u001b[01;34mgtCoarse\u001b[00m\r\n",
      "│   ├── \u001b[01;34mtrain\u001b[00m\r\n",
      "│   ├── \u001b[01;34mtrain_extra\u001b[00m\r\n",
      "│   └── \u001b[01;34mval\u001b[00m\r\n",
      "├── \u001b[01;34mgtFine\u001b[00m\r\n",
      "│   ├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   ├── \u001b[01;34mtrain\u001b[00m\r\n",
      "│   └── \u001b[01;34mval\u001b[00m\r\n",
      "└── \u001b[01;34mleftImg8bit\u001b[00m\r\n",
      "    ├── \u001b[01;34mtest\u001b[00m\r\n",
      "    ├── \u001b[01;34mtrain\u001b[00m\r\n",
      "    ├── \u001b[01;34mtrain_extra\u001b[00m\r\n",
      "    └── \u001b[01;34mval\u001b[00m\r\n",
      "\r\n",
      "13 directories\r\n"
     ]
    }
   ],
   "source": [
    "!tree -d -L 2 /home/skywatcher/datasets/data/cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-21T16:05:31.413Z"
    }
   },
   "outputs": [],
   "source": [
    "images = '/home/skywatcher/datasets/data/cityscapes/train/images'\n",
    "masks = '/home/skywatcher/datasets/data/cityscapes/train/masks'\n",
    "val_images = '/home/skywatcher/datasets/data/cityscapes/val/images'\n",
    "val_masks = '/home/skywatcher/datasets/data/cityscapes/train/masks'\n",
    "train_generator, val_generator = cs_create_generators(images, masks, val_images, val_masks, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
